# -*- coding: utf-8 -*-
"""pretraining_retinopatia_diabetica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8Faq9n8WX13tBf5cOAqzcBbN5HakBNf
"""

# =========================================
# CELLA 1 — Config, IMPORT UNIFICATI, setup
# =========================================

# ---- Config modificabili ----
USE_WANDB        = True        # grafici Weights & Biases (False per disattivare)
EPOCHS           = 30          # per prova rapida 10–15, poi 30–50
BATCH_SIZE       = 32
IMG_SIZE         = (224, 224)
SEED             = 42
LABEL_COL        = "level"
TFRECORD_SHARDS  = 512         # 256–512 per ~38 GB
WARMUP_PCT       = 0.05

# ---- Import TUTTI QUI ----
import os, sys, io, re, glob, math, json, random, hashlib, shutil, tempfile, subprocess, importlib.util
from datetime import datetime
import zipfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from collections import Counter
from sklearn.model_selection import GroupShuffleSplit, train_test_split
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import mixed_precision

# ---- Setup runtime ----
print("TensorFlow:", tf.__version__)
# GPU memory growth
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for g in gpus: tf.config.experimental.set_memory_growth(g, True)
        print("GPU memory growth ON")
    except Exception as e:
        print("Warn set_memory_growth:", e)
# XLA
try:
    tf.config.optimizer.set_jit(True); print("XLA attivato.")
except Exception as e:
    print("XLA non disponibile:", e)
# Mixed precision
mixed_precision.set_global_policy('mixed_float16'); print("Mixed precision attivata.")
np.random.seed(SEED); random.seed(SEED); tf.keras.utils.set_random_seed(SEED)

# ---- Mount Drive & auto-locate archivi ----
try:
    assert os.path.isdir("/content/drive")
except AssertionError:
    from google.colab import drive
    drive.mount('/content/drive')

assert os.path.isdir("/content/drive"), "❌ Drive non montato."

def _pick_best(cands):
    if not cands: return None
    prefer = [c for c in cands if "/DueDataset/" in c]
    return (prefer or cands)[0]

candidates_zip1 = sorted(glob.glob("/content/drive/**/*train_images.zip", recursive=True))
candidates_zip2 = sorted(glob.glob("/content/drive/**/*train_full.7z",   recursive=True))
ZIP1 = _pick_best(candidates_zip1)
ZIP2 = _pick_best(candidates_zip2)

if ZIP1 is None or ZIP2 is None:
    print("Possibili train_images.zip:", candidates_zip1[:5])
    print("Possibili train_full.7z:",   candidates_zip2[:5])
    raise FileNotFoundError("❌ Non trovo uno o entrambi gli archivi su /content/drive/ …verifica i nomi/posizioni.")

DRIVE_ROOT = os.path.dirname(ZIP1)     # cartella dove sta lo zip APTOS
DRIVE_DST  = os.path.join(DRIVE_ROOT, "extracted")
TFREC_DIR  = os.path.join(DRIVE_DST, "tfrecords")
os.makedirs(TFREC_DIR, exist_ok=True)

print("✅ Archivio APTOS:", ZIP1)
print("✅ Archivio DR   :", ZIP2)
print("DRIVE_ROOT      :", DRIVE_ROOT)
print("TFREC_DIR       :", TFREC_DIR)

# ---- Helper: py7zr e lettura robusta da 7z ----
def ensure_py7zr():
    """Installa e importa py7zr se mancante; ritorna il modulo."""
    if importlib.util.find_spec("py7zr") is None:
        print("Installo py7zr…")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "py7zr"])
    import py7zr
    return py7zr

def seven_read_file(a7z, inner_path: str) -> bytes:
    """
    Leggi un singolo file dal 7z. Prima prova in memoria (readall),
    poi fallback: estrai solo quel file su una temp dir e leggi i bytes.
    """
    if hasattr(a7z, "readall"):
        out = a7z.readall([inner_path])  # dict name -> BytesIO
        bio = out.get(inner_path, None)
        if bio is not None:
            return bio.read()
    tmpdir = tempfile.mkdtemp(prefix="_one7z_")
    try:
        a7z.extract(targets=[inner_path], path=tmpdir)
        fp = os.path.join(tmpdir, inner_path)
        with open(fp, "rb") as f:
            return f.read()
    finally:
        shutil.rmtree(tmpdir, ignore_errors=True)

# ---- Helper: (opzionale) W&B lazy installer ----
def ensure_wandb():
    if importlib.util.find_spec("wandb") is None:
        print("Installo wandb…")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "--upgrade", "wandb"])
    import wandb
    from wandb.integration.keras import WandbCallback
    return wandb, WandbCallback

print("✅ Setup & import completati. Prosegui con la Cella 2.")

# ===================================================================
# CELLA 2 — FAST-TRACK: TFRecord direttamente da .zip/.7z (GZIP)
# ===================================================================
py7zr = ensure_py7zr()

# --- APTOS (ZIP): indicizza immagini e leggi train.csv ---
with zipfile.ZipFile(ZIP1, 'r') as z:
    zip_members = z.namelist()
    zip_index = {}
    for name in zip_members:
        low = name.lower()
        if low.endswith(('.png','.jpg','.jpeg')):
            base = os.path.splitext(os.path.basename(name))[0].lower()
            zip_index[base] = name
    csv_candidates = [n for n in zip_members if n.lower().endswith('train.csv')]
    assert csv_candidates, "❌ train.csv non trovato nello zip APTOS."
    with z.open(csv_candidates[0]) as f:
        df1 = pd.read_csv(f)

# --- DR (7z): indicizza immagini e leggi trainLabels.csv (anche se 'cartella') ---
with py7zr.SevenZipFile(ZIP2, mode='r') as a:
    seven_members = a.getnames()
    seven_index = {}
    for name in seven_members:
        low = name.lower()
        if '/train_full/' in low and low.endswith(('.png','.jpg','.jpeg')):
            base = os.path.splitext(os.path.basename(name))[0].lower()
            seven_index[base] = name
    csv2_candidates = [n for n in seven_members if n.lower().endswith('trainlabels.csv')]
    assert csv2_candidates, "❌ trainLabels.csv non trovato nel 7z DR."
    csv_inner = csv2_candidates[0]
    csv_bytes = seven_read_file(a, csv_inner)
    df2 = pd.read_csv(io.BytesIO(csv_bytes))

# --- Normalizza colonne immagine/label ---
img1 = 'id_code' if 'id_code' in df1.columns else ('image' if 'image' in df1.columns else None)
lab1 = 'diagnosis' if 'diagnosis' in df1.columns else ('level' if 'level' in df1.columns else None)
img2 = 'image' if 'image' in df2.columns else ('id_code' if 'id_code' in df2.columns else None)
lab2 = 'level' if 'level' in df2.columns else ('diagnosis' if 'diagnosis' in df2.columns else None)
assert img1 and lab1 and img2 and lab2, "❌ Colonne immagine/label non trovate nei CSV."

df1 = df1.copy(); df2 = df2.copy()
df1[LABEL_COL] = df1[lab1].astype(str)
df2[LABEL_COL] = df2[lab2].astype(str)

# --- Class mapping globale ---
all_labels = pd.concat([df1[LABEL_COL], df2[LABEL_COL]], axis=0).astype(str)
class_names = sorted(all_labels.unique().tolist())
class_to_idx = {c:i for i,c in enumerate(class_names)}
num_classes  = len(class_names)
print("Classi:", class_to_idx)

# --- Righe “virtuali” (senza leggere immagini) ---
def df_to_rows(df, src, image_col, label_col):
    rows = []
    for name, lab in df[[image_col, label_col]].itertuples(index=False):
        base = os.path.splitext(str(name))[0].lower()
        rows.append(dict(__src__=src, __name__=base, __label__=class_to_idx[str(lab)]))
    return pd.DataFrame(rows)

rows1 = df_to_rows(df1, 'aptos', img1, LABEL_COL)
rows2 = df_to_rows(df2, 'dr',    img2, LABEL_COL)
rows  = pd.concat([rows1, rows2], ignore_index=True)

# --- Split per paziente virtuale (no leakage) ---
def infer_patient_id_virtual(src, name):
    if src == 'dr':
        norm = re.sub(r"(_left|_right|_l|_r|_os|_od|_oi|_oe)$", "", name)
        norm = re.sub(r"_[01]$", "", norm)
        return f"dr:{norm}"
    else:
        return f"aptos:{name}"

rows['patient'] = [infer_patient_id_virtual(s, n) for s, n in zip(rows['__src__'], rows['__name__'])]

if rows['patient'].nunique() < len(rows):
    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)
    tr_idx, va_idx = next(gss.split(rows, groups=rows['patient']))
    train_rows, val_rows = rows.iloc[tr_idx], rows.iloc[va_idx]
else:
    train_rows, val_rows = train_test_split(rows, test_size=0.2, random_state=SEED, stratify=rows['__label__'])

# Anti-leakage
overlap = set(train_rows['patient']).intersection(set(val_rows['patient']))
assert len(overlap) == 0, f"Leakage: {len(overlap)} patient-id in comune!"
print(f"Split → train={len(train_rows)}  val={len(val_rows)}")

# --- Class weights dal train ---
cnt = Counter(train_rows['__label__'].values); tot = sum(cnt.values())
class_weight = {k: tot/(num_classes*cnt[k]) for k in cnt}
print("Class weights:", class_weight)

# --- Writer TFRecord (gzip) shardato ---
TFREC_OPTS = tf.io.TFRecordOptions(compression_type="GZIP")
def _bytes(x): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[x]))
def _int(x):   return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(x)]))

def write_sharded_tfrecord(rows, split_name, shards=TFRECORD_SHARDS):
    parts = np.array_split(rows.reset_index(drop=True), max(1, min(shards, len(rows))))
    for i, part in enumerate(parts):
        out = os.path.join(TFREC_DIR, f"{split_name}_{i:04d}.tfrec.gz")
        if os.path.isfile(out):  # idempotente
            continue
        with tf.io.TFRecordWriter(out, options=TFREC_OPTS) as w, \
             zipfile.ZipFile(ZIP1, 'r') as z, \
             py7zr.SevenZipFile(ZIP2, mode='r') as a:
            for _, row in part.iterrows():
                src, name, label = row['__src__'], row['__name__'], int(row['__label__'])
                try:
                    if src == 'aptos':
                        inner = zip_index.get(name) or zip_index.get(os.path.splitext(name)[0])
                        if inner is None:
                            continue
                        img_bytes = z.read(inner)
                        ext = os.path.splitext(inner)[1].lower().encode()
                    else:
                        inner = seven_index.get(name) or seven_index.get(os.path.splitext(name)[0])
                        if inner is None:
                            continue
                        img_bytes = seven_read_file(a, inner)
                        ext = os.path.splitext(inner)[1].lower().encode()
                except Exception:
                    continue
                ex = tf.train.Example(features=tf.train.Features(feature={
                    'image': _bytes(img_bytes),
                    'label': _int(label),
                    'ext':   _bytes(ext),
                }))
                w.write(ex.SerializeToString())

need_write = (not glob.glob(os.path.join(TFREC_DIR, "train_*.tfrec.gz"))) or \
             (not glob.glob(os.path.join(TFREC_DIR, "val_*.tfrec.gz")))
if need_write:
    print(">> Scrittura TFRecord (gzip, sharded)…")
    write_sharded_tfrecord(train_rows, "train", shards=TFRECORD_SHARDS)
    write_sharded_tfrecord(val_rows,   "val",   shards=max(1, TFRECORD_SHARDS//4))
else:
    print("TFRecord già presenti — salto scrittura.")

TRAIN_SAMPLES = int(len(train_rows))
print("✅ TFRecord pronti in:", TFREC_DIR)
print("✅ num_classes:", num_classes, "| TRAIN_SAMPLES:", TRAIN_SAMPLES)

# ============================================================
# CELLA 3 — Loader TFRecord: creazione dataset train/val
# ============================================================
import tensorflow as tf
import os, glob

print("✅ Carico TFRecord da:", TFREC_DIR)

# Parser singolo record
def _parse_fn(proto):
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64),
        'ext':   tf.io.FixedLenFeature([], tf.string),
    }
    example = tf.io.parse_single_example(proto, feature_description)
    img = tf.image.decode_image(example['image'], channels=3, expand_animations=False)
    img = tf.image.resize(img, IMG_SIZE)
    img = tf.cast(img, tf.float32) / 255.0
    label = tf.one_hot(tf.cast(example['label'], tf.int32), num_classes)
    return img, label

# Costruzione dataset
def make_tfrec_ds(pattern, batch_size, shuffle=False):
    files = sorted(glob.glob(pattern))
    ds = tf.data.TFRecordDataset(files, compression_type="GZIP", num_parallel_reads=tf.data.AUTOTUNE)
    ds = ds.map(_parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        ds = ds.shuffle(buffer_size=batch_size * 10, seed=SEED, reshuffle_each_iteration=True)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

train_ds = make_tfrec_ds(os.path.join(TFREC_DIR, "train_*.tfrec.gz"), BATCH_SIZE, shuffle=True)
val_ds   = make_tfrec_ds(os.path.join(TFREC_DIR, "val_*.tfrec.gz"),   BATCH_SIZE, shuffle=False)

xb, yb = next(iter(train_ds.take(1)))
print("✅ batch:", xb.shape, yb.shape, xb.dtype, yb.dtype)

# ============================================================
# CELLA 4 — Costruzione modello (ResNet50) + LR warmup→cosine
# con steps_per_epoch ROBUSTO (cardinality -> fallback)
# ============================================================
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
import tensorflow as tf, math

def _steps_per_epoch_from_ds(ds, default_samples, batch_size):
    """Restituisce steps/epoch robusto:
       1) usa ds.cardinality() se è finita;
       2) altrimenti fallback a ceil(default_samples / batch_size)."""
    try:
        card = int(ds.cardinality().numpy())
        if 0 < card < 2**60:  # valore finito e sensato
            return card
    except Exception:
        pass
    return max(1, math.ceil(default_samples / batch_size))

def build_model_with_warmup(epochs=EPOCHS):
    inputs = layers.Input(shape=(*IMG_SIZE, 3))

    # Augmentations on-graph (riproducibili con SEED)
    aug = tf.keras.Sequential([
        layers.RandomFlip("horizontal", seed=SEED),
        layers.RandomRotation(0.05, seed=SEED, fill_mode='reflect'),
        layers.RandomZoom(0.1, seed=SEED),
        layers.RandomContrast(0.1, seed=SEED),
    ], name="data_aug")(inputs)

    x = layers.Rescaling(1./255)(aug)

    # Backbone from scratch
    base = ResNet50(include_top=False, weights=None, input_tensor=x)
    x = layers.GlobalAveragePooling2D()(base.output)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(1024, activation='relu', dtype='float32')(x)
    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)

    model = models.Model(inputs, outputs)

    # ✅ steps_per_epoch robusto
    steps_per_epoch = _steps_per_epoch_from_ds(
        train_ds, default_samples=TRAIN_SAMPLES, batch_size=BATCH_SIZE
    )
    total_steps  = steps_per_epoch * epochs
    warmup_steps = max(1, int(0.05 * total_steps))

    # LR schedule: warmup → cosine
    cosine = tf.keras.optimizers.schedules.CosineDecay(
        initial_learning_rate=3e-4,
        decay_steps=max(1, total_steps - warmup_steps),
        alpha=0.1
    )
    def lr_with_warmup(step):
        step = tf.cast(step, tf.float32)
        return tf.where(step < warmup_steps,
                        3e-4 * (step / tf.cast(warmup_steps, tf.float32)),
                        cosine(step - warmup_steps))

    opt = tf.keras.optimizers.AdamW(
        learning_rate=lr_with_warmup,
        weight_decay=1e-4,
        global_clipnorm=1.0
    )

    model.compile(
        optimizer=opt,
        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),
        metrics=[
            tf.keras.metrics.CategoricalAccuracy(name='acc'),
            tf.keras.metrics.AUC(name='auc', multi_label=True, num_labels=num_classes),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
        ]
    )

    print(f"[INFO] steps_per_epoch={steps_per_epoch}, warmup_steps={warmup_steps}, total_steps={total_steps}")
    return model

model = build_model_with_warmup()
model.summary()
print("✅ Modello compilato.")

# ============================================================
# CELLA 5 — EMA + W&B + callbacks
# ============================================================
import os, math, datetime, numpy as np, tensorflow as tf

# batches/epoch robusto (cardinality -> fallback)
def _batches_per_epoch():
    try:
        card = int(train_ds.cardinality().numpy())
        if 0 < card < 2**60:
            return card
    except Exception:
        pass
    return max(1, math.ceil(TRAIN_SAMPLES / BATCH_SIZE))

# ---- EMA callback (valuta con pesi mediati a fine epoca)
class EMACallback(tf.keras.callbacks.Callback):
    def __init__(self, decay, val_dataset):
        super().__init__(); self.decay, self.val_dataset = decay, val_dataset
    def on_train_begin(self, logs=None):
        # Inizializza ema_vars come tf.Variable per poter usare .assign()
        self.ema_vars = [tf.Variable(v) for v in self.model.trainable_variables]
    @tf.function
    def _upd(self):
        for ema, var in zip(self.ema_vars, self.model.trainable_variables):
            # Usa ema.assign per aggiornare la variabile EMA
            ema.assign(self.decay * ema + (1.0 - self.decay) * var)
    def on_train_batch_end(self, batch, logs=None):
        self._upd()
    def on_epoch_end(self, epoch, logs=None):
        # backup -> swap EMA -> eval -> restore
        backup = [tf.identity(v) for v in self.model.trainable_variables]
        for v, e in zip(self.model.trainable_variables, self.ema_vars): v.assign(e)
        res = self.model.evaluate(self.val_dataset, verbose=0, return_dict=True)
        logs.update({f"val_ema_{k}": v for k, v in res.items()})
        print(f"[EMA] epoch {epoch+1}:", {f"val_ema_{k}":v for k,v in res.items()})
        for v, b in zip(self.model.trainable_variables, backup): v.assign(b)


ema_decay_per_step = float(np.exp(np.log(0.5) / _batches_per_epoch()))
print(f"[INFO] EMA decay/step={ema_decay_per_step:.6f} | batches/epoch={_batches_per_epoch()}")


# ---- W&B (opzionale, controllato da USE_WANDB)
wandb_cb = None
if 'USE_WANDB' in globals() and USE_WANDB:
    try:
        import wandb
        from wandb.integration.keras import WandbCallback
        # Check if wandb run is already initialized
        if wandb.run is None:
            wandb.login(timeout=30)
            wandb.init(project="Tesi-Ocular-Pretraining",
                       config=dict(epochs=EPOCHS, batch_size=BATCH_SIZE,
                                   img_size=IMG_SIZE, num_classes=num_classes))
        # Disable graph saving as it seems to cause an issue
        wandb_cb = WandbCallback(save_model=False, monitor="val_ema_loss", save_graph=False)
        print("✅ W&B attivo.")
    except Exception as e:
        print("[WARN] W&B offline:", e)

# ---- cartella esperimento + callbacks
OUT_DIR = os.path.join(DRIVE_ROOT, f"esperimenti/run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}")
os.makedirs(OUT_DIR, exist_ok=True)
CKPT = os.path.join(OUT_DIR, "best.weights.h5")

callbacks = [
    EMACallback(decay=ema_decay_per_step, val_dataset=val_ds),
    tf.keras.callbacks.ModelCheckpoint(filepath=CKPT, monitor='val_ema_loss',
                                       save_best_only=True, save_weights_only=True, mode='min', verbose=1),
    tf.keras.callbacks.EarlyStopping(monitor='val_ema_loss', patience=8,
                                     mode='min', restore_best_weights=False, verbose=1),
]
if wandb_cb: callbacks.append(wandb_cb)
print("✅ Callbacks pronti. CKPT:", CKPT)

# ============================
# PATCH LR: Warmup + Cosine (dtype fix) + Recompile
# ============================
import math, tensorflow as tf

# 1) steps/epoch dal conteggio noto (robusto)
steps_per_epoch = max(1, math.ceil(TRAIN_SAMPLES / BATCH_SIZE))
total_steps     = steps_per_epoch * EPOCHS
warmup_steps    = max(1, int(0.05 * total_steps))
base_lr         = 3e-4
alpha           = 0.1  # lr finale = alpha * base_lr

# 2) Schedule con dtype coerenti (int64 per confronti)
class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, base_lr, total_steps, warmup_steps=0, alpha=0.0, name=None):
        super().__init__()
        self.base_lr      = tf.convert_to_tensor(base_lr, dtype=tf.float32)
        self.total_steps  = tf.convert_to_tensor(int(total_steps), dtype=tf.int64)
        self.warmup_steps = tf.convert_to_tensor(int(warmup_steps), dtype=tf.int64)
        self.alpha        = tf.convert_to_tensor(alpha, dtype=tf.float32)
        self.name         = name or "WarmupCosine"

    def __call__(self, step):
        step = tf.cast(step, tf.int64)

        # warmup lineare (denominatore in float32)
        warmup_den = tf.maximum(
            tf.constant(1.0, dtype=tf.float32),
            tf.cast(self.warmup_steps, tf.float32)
        )
        warmup_lr = self.base_lr * tf.cast(step, tf.float32) / warmup_den

        # dopo warmup: cosine decay
        zero64 = tf.constant(0, dtype=tf.int64)
        one64  = tf.constant(1, dtype=tf.int64)

        post   = tf.maximum(zero64, step - self.warmup_steps)                     # int64 OK
        denom  = tf.maximum(one64,  self.total_steps - self.warmup_steps)         # int64 OK
        progress = tf.cast(post,  tf.float32) / tf.cast(denom, tf.float32)        # float32

        cosine_lr = (
            self.alpha * self.base_lr
            + 0.5 * (self.base_lr - self.alpha * self.base_lr)
              * (1.0 + tf.cos(tf.constant(math.pi, dtype=tf.float32) * progress))
        )

        return tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)

    def get_config(self):
        return {
            "base_lr": float(self.base_lr.numpy()),
            "total_steps": int(self.total_steps.numpy()),
            "warmup_steps": int(self.warmup_steps.numpy()),
            "alpha": float(self.alpha.numpy()),
            "name": self.name,
        }

# 3) Istanzia schedule + recompile (manteniamo architettura e pesi)
lr_schedule = WarmupCosine(
    base_lr=base_lr,
    total_steps=total_steps,
    warmup_steps=warmup_steps,
    alpha=alpha
)

opt = tf.keras.optimizers.AdamW(
    learning_rate=lr_schedule,
    weight_decay=1e-4,
    global_clipnorm=1.0
)

model.compile(
    optimizer=opt,
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),
    metrics=[
        tf.keras.metrics.CategoricalAccuracy(name='acc'),
        tf.keras.metrics.AUC(name='auc', multi_label=True, num_labels=num_classes),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
    ]
)

print(f"✔️ Recompile OK | steps/epoch={steps_per_epoch}, warmup_steps={warmup_steps}, total_steps={total_steps}")
print("LR schedule:", type(model.optimizer.learning_rate))

# ============================================================
# CELLA 6 — Addestramento
# ============================================================
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    class_weight=class_weight,
    callbacks=callbacks,
    verbose=1
)
print("✅ Addestramento completato.")

# ============================================================
# CELLA 7 — Salvataggio pesi migliori (EMA) + plot
# ============================================================
import os, json, matplotlib.pyplot as plt

# carica best checkpoint e salva pesi finali su Drive
model.load_weights(CKPT)
FINAL_W = os.path.join(DRIVE_ROOT, "best_ocular_pretrained_combined_ema.weights.h5")
model.save_weights(FINAL_W)
print("✅ Pesi finali salvati in:", FINAL_W)

# plot metriche
hist = history.history
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(hist.get('acc',[])); plt.plot(hist.get('val_acc',[])); plt.plot(hist.get('val_ema_acc',[]),'--')
plt.title('Accuracy'); plt.legend(['train','val','val_ema'])
plt.subplot(1,2,2)
plt.plot(hist.get('loss',[])); plt.plot(hist.get('val_loss',[])); plt.plot(hist.get('val_ema_loss',[]),'--')
plt.title('Loss'); plt.legend(['train','val','val_ema'])
plt.tight_layout(); plt.show()

# metadata essenziali
meta = dict(epochs=EPOCHS, batch_size=BATCH_SIZE, num_classes=num_classes,
            ema_decay_per_step=ema_decay_per_step)
with open(os.path.join(OUT_DIR, "run.json"), "w") as f:
    json.dump(meta, f, indent=2)
print("📄 Metadata salvati in:", OUT_DIR)
print("🎉 Fine pretraining — pesi pronti per la Fase 2.")